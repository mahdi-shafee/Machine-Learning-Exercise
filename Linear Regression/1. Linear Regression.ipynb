{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16adc323",
   "metadata": {},
   "source": [
    "<br><font face=\"Times New Roman\" size=5><div dir=ltr align=center>\n",
    "<font color=blue size=8>\n",
    "    Introduction to Machine Learning <br>\n",
    "<font color=red size=5>\n",
    "    Sharif University of Technology - Computer Engineering Department <br>\n",
    "    Fall 2022<br> <br>\n",
    "<font color=black size=6>\n",
    "    Homework 2: Practical - Linear Regression\n",
    "    </div>\n",
    "<br><br>\n",
    "<font size=4>\n",
    "   **Name**: Mahdi Shafiee<br>\n",
    "   **Student ID**: 99109409<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585264a",
   "metadata": {},
   "source": [
    "<font face=\"Times New Roman\" size=4><div dir=ltr>\n",
    "# Problem 1: Linear Regression Model (40 + 30 optional points)\n",
    "According to <a href=\"https://github.com/asharifiz/Introduction_to_Machine_Learning/blob/main/Jupyter_Notebooks/Chapter_02_Classical_Models/Linear%20regression.ipynb\"><font face=\"Roboto\">Linear Regression Notebook</font></a>, train a linear regression model on an arbitrary dataset. Explain your chosen dataset and split your data into train and test sets, then predict values for the test set using your trained model. Try to find the best hyperparameters for your model. (Using Lasso Regression, Ridge Regression or Elastic Net and comparing them will have extra optional points)\n",
    "<br> Explain each step of your workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d282a5f",
   "metadata": {},
   "source": [
    "First we make our dataset using \"make_regression\" function from \"sklearn\" library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98435b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2952a1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_regression(n_samples=50000, n_features=8, noise=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c66d6bf7",
   "metadata": {},
   "source": [
    "Now we transform y from an array to a n by 1 matrix and normalize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f18ccad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27328a3e",
   "metadata": {},
   "source": [
    "Then, we add the bias feature by adding a column of ones to the matrix X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a3126d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = np.ones([len(X), 1])\n",
    "X = np.hstack((ones, X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de914c4d",
   "metadata": {},
   "source": [
    "For minimizing the Loss Function, we will use gradient descent. Also both Lasso regularization and Ridge regularization will be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bee9dbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_cost(X, y, w, l):\n",
    "    m = len(X)\n",
    "    error = y - np.dot(X, w)\n",
    "    return 1 / (2 * m) * np.dot(error.T, error) + l / (2 * m) * np.dot(w.T, w)\n",
    "\n",
    "\n",
    "def ridge_gradient(X, y, w, l):\n",
    "    m = len(X)\n",
    "    error = y - np.dot(X, w)\n",
    "    return -1 / m * np.dot(X.T, error) + l * w / m\n",
    "\n",
    "\n",
    "def lasso_cost(X, y, w, l):\n",
    "    m = len(X)\n",
    "    error = y - np.dot(X, w)\n",
    "    return 1 / (2 * m) * np.dot(error.T, error) + l / m * sum(abs(w))\n",
    "\n",
    "\n",
    "def lasso_gradient(X, y, w, l):\n",
    "    m = len(X)\n",
    "    error = y - np.dot(X, w)\n",
    "    return -1 / m * np.dot(X.T, error) + l * np.sign(w) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32fb64f",
   "metadata": {},
   "source": [
    "Here the initial values of costs and gradients are shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff10958e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial cost: 9931.959850569208\n"
     ]
    }
   ],
   "source": [
    "w = np.random.rand(len(X[0]), 1)\n",
    "l = 0\n",
    "print(\"initial cost:\", ridge_cost(X, y, w, l)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb42ec0",
   "metadata": {},
   "source": [
    "First, regularization factor $\\lambda$ is set to $0$ and step length is set to $0.01$ and number of steps is set to $1000$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c49c4e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 0.01\n",
    "step_number = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4941a030",
   "metadata": {},
   "source": [
    "Gradient Descent algorithm (without regularization) has been implemented below.\n",
    "As we run Gradient Descent, cost at each step is stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7d0f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "step = []\n",
    "costs = []\n",
    "def ridge_gradient_descent(X, y, w, l, delta, step_number):\n",
    "    for i in range(step_number):\n",
    "        w = w - delta * ridge_gradient(X, y, w, l)\n",
    "        step.append(i)\n",
    "        costs.append(ridge_cost(X, y, w, l)[0])\n",
    "    return w\n",
    "\n",
    "\n",
    "def lasso_gradient_descent(X, y, w, l, delta, step_number):\n",
    "    for i in range(step_number):\n",
    "        w = w - delta * lasso_gradient(X, y, w, l)\n",
    "        step.append(i)\n",
    "        costs.append(ridge_cost(X, y, w, l)[0])\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a3f654d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimum w (using gradient descent):\n",
      "[[-4.70370413e-02]\n",
      " [ 2.16538039e+01]\n",
      " [ 2.99367342e+01]\n",
      " [ 8.73294133e+01]\n",
      " [ 1.12269259e+01]\n",
      " [ 2.86406117e+01]\n",
      " [ 6.20816607e+01]\n",
      " [ 6.36790666e+01]\n",
      " [ 4.79697362e+01]]\n",
      "\n",
      "final cost: 50.13308561841004\n"
     ]
    }
   ],
   "source": [
    "new_w = ridge_gradient_descent(X, y, w, l, delta, step_number)\n",
    "print(\"optimum w (using gradient descent):\")\n",
    "print(new_w)\n",
    "print()\n",
    "print(\"final cost:\", ridge_cost(X, y, new_w, l)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1be9b",
   "metadata": {},
   "source": [
    "To check our implementation, closed form answer of linear regression is shown below. Fortunately, both answers are pretty similar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cf0584c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimum w (using closed form answer):\n",
      "[[-4.72418539e-02]\n",
      " [ 2.16548186e+01]\n",
      " [ 2.99373302e+01]\n",
      " [ 8.73338228e+01]\n",
      " [ 1.12272341e+01]\n",
      " [ 2.86418502e+01]\n",
      " [ 6.20840321e+01]\n",
      " [ 6.36819130e+01]\n",
      " [ 4.79714978e+01]]\n",
      "50.13306607214577\n"
     ]
    }
   ],
   "source": [
    "p = np.linalg.inv(np.dot(X.T, X))\n",
    "q = np.dot(X.T, y)\n",
    "print(\"optimum w (using closed form answer):\")\n",
    "print(np.dot(p, q))\n",
    "print(ridge_cost(X, y, np.dot(p, q), l)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f0068",
   "metadata": {},
   "source": [
    "As the cost-step plot indicates, step number of 5000 is good enough, because the algorithm has pretty much converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "632c0358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(step, np.log(costs))\n",
    "plt.title(\"logarithmic cost-step function for unregularized gradient descent\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd97abde",
   "metadata": {},
   "source": [
    "In the next step, we compare different amounts of regularization factor for Ridge Regression and Lasso Regression. For calculating best regularization factor, K-fold Cross Validation is used.\n",
    "As results show below, our model doesn't suffer from overfitting in the first place, so increasing $\\lambda$ reduces the error negligibly.<br>\n",
    "As we see below, the least error for test data is obtained when $\\lambda$ is approximately equal to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f01347ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ridge regularization:\n",
      "\n",
      "accuracy of validation set for lambda=0: 50.31940532528792\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=5: 50.31659569686156\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=10: 50.31410726631459\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=20: 50.31009303559122\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=50: 50.30573407994907\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=100: 50.32396664159303\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=200: 50.4551911820718\n",
      "------------------------------------------------\n",
      "\n",
      "best results:\n",
      "least error: 50.30573407994907\n",
      "best lambda: 50\n"
     ]
    }
   ],
   "source": [
    "m = len(X) // 5 * 4\n",
    "X_train = X[:m, :]\n",
    "y_train = y[:m, :]\n",
    "X_test = X[m:, :]\n",
    "y_test = y[m:, :]\n",
    "\n",
    "delta = 0.01\n",
    "step_number = 1000\n",
    "\n",
    "print(\"ridge regularization:\\n\")\n",
    "\n",
    "l = [0, 5, 10, 20, 50, 100, 200]\n",
    "\n",
    "least_error = 1000000000\n",
    "best_l = 0\n",
    "\n",
    "for i in range(len(l)):\n",
    "    w_new = ridge_gradient_descent(X_train, y_train, w, l[i], delta, step_number)\n",
    "    new_error = ridge_cost(X_test, y_test, w_new, 0)[0][0]\n",
    "    if new_error < least_error:\n",
    "        least_error = new_error\n",
    "        best_l = l[i]\n",
    "    print(f\"accuracy of validation set for lambda={l[i]}: {new_error}\")\n",
    "    print(\"------------------------------------------------\")\n",
    "\n",
    "print(\"\\nbest results:\")\n",
    "print(\"least error:\", least_error)\n",
    "print(\"best lambda:\", best_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf9ffb4",
   "metadata": {},
   "source": [
    "As we have shown below, for this dataset, lasso regularization doesn't work very well. Best regularization factor is nearly zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8b111bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lasso regularization:\n",
      "\n",
      "accuracy of validation set for lambda=0: 50.31940532528792\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=10: 50.31931391621164\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=20: 50.31922307302135\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=50: 50.318953939708805\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=100: 50.31851670352769\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=200: 50.317684682883936\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=500: 50.31552826017359\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=1000: 50.31306650172683\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=2000: 50.312389212578395\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=5000: 50.340035511710035\n",
      "------------------------------------------------\n",
      "accuracy of validation set for lambda=10000: 50.48696603853041\n",
      "------------------------------------------------\n",
      "\n",
      "best results:\n",
      "least error: 50.312389212578395\n",
      "best lambda: 2000\n"
     ]
    }
   ],
   "source": [
    "m = len(X) // 5 * 4\n",
    "X_train = X[:m, :]\n",
    "y_train = y[:m, :]\n",
    "X_test = X[m:, :]\n",
    "y_test = y[m:, :]\n",
    "\n",
    "delta = 0.01\n",
    "step_number = 1000\n",
    "\n",
    "print(\"lasso regularization:\\n\")\n",
    "\n",
    "l = [0, 10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "\n",
    "least_error = 1000000000\n",
    "best_l = 0\n",
    "\n",
    "for i in range(len(l)):\n",
    "    w_new = lasso_gradient_descent(X_train, y_train, w, l[i], delta, step_number)\n",
    "    new_error = lasso_cost(X_test, y_test, w_new, 0)[0][0]\n",
    "    if new_error < least_error:\n",
    "        least_error = new_error\n",
    "        best_l = l[i]\n",
    "    print(f\"accuracy of validation set for lambda={l[i]}: {new_error}\")\n",
    "    print(\"------------------------------------------------\")\n",
    "\n",
    "print(\"\\nbest results:\")\n",
    "print(\"least error:\", least_error)\n",
    "print(\"best lambda:\", best_l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
